# exploits/dir_enum.py

import aiohttp
import asyncio
import logging
from typing import List, Optional, Set
from urllib.parse import urljoin

DEFAULT_HEADERS = {
    "User-Agent": ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                   "AppleWebKit/537.36 (KHTML, like Gecko) "
                   "Chrome/100.0.4896.127 Safari/537.36")
}

COMMON_DIR_WORDLIST = [
    "admin", "login", "dashboard", "config", "server-status", "backup",
    "uploads", "files", "cgi-bin", "test", "old", "portal", "manage",
    "wp-admin", "xmlrpc.php", "shell", "console", "api", "private"
]

async def test_directory(session: aiohttp.ClientSession, base_url: str, directory: str,
                         proxy: Optional[str] = None, headers: dict = DEFAULT_HEADERS,
                         delay: float = 0) -> Optional[str]:
    target_url = urljoin(base_url, directory)
    try:
        async with session.get(target_url, headers=headers, proxy=proxy, ssl=False) as resp:
            text = await resp.text(errors="ignore")
            if resp.status in [200, 301, 302]:
                # Basic content analysis to check if likely directory or index page
                indicators = ["index of", "<title>index", "Directory listing", "parent directory"]
                if any(indicator.lower() in text.lower() for indicator in indicators) or directory.endswith("/") or resp.status != 200:
                    logging.info(f"[DIR-ENUM] Found directory: {target_url} (Status: {resp.status})")
                    if delay > 0:
                        await asyncio.sleep(delay)
                    return target_url
    except Exception as e:
        logging.debug(f"[DIR-ENUM] Error testing {target_url}: {e}")
    if delay > 0:
        await asyncio.sleep(delay)
    return None

async def recursive_dir_enum(session: aiohttp.ClientSession, base_url: str,
                            wordlist: List[str], max_depth: int, current_depth: int = 0,
                            found_dirs: Set[str] = None, proxy: Optional[str] = None,
                            headers: dict = DEFAULT_HEADERS, delay: float = 0) -> Set[str]:

    if found_dirs is None:
        found_dirs = set()

    if current_depth > max_depth:
        return found_dirs

    tasks = []
    new_dirs = set()
    for dir_name in wordlist:
        # Ensure directory ends with /
        dir_candidate = dir_name if dir_name.endswith("/") else dir_name + "/"
        # Avoid duplicates
        full_url = urljoin(base_url, dir_candidate)
        if full_url in found_dirs:
            continue
        tasks.append(test_directory(session, base_url, dir_candidate, proxy=proxy, headers=headers, delay=delay))

    results = await asyncio.gather(*tasks, return_exceptions=True)
    for res in results:
        if isinstance(res, str) and res:
            new_dirs.add(res)

    found_dirs.update(new_dirs)

    # Recurse into new directories found
    for new_dir in new_dirs:
        await recursive_dir_enum(session, new_dir, wordlist, max_depth, current_depth + 1,
                                found_dirs, proxy, headers, delay)

    return found_dirs

async def run_dir_enum(base_url: str, wordlist: List[str] = COMMON_DIR_WORDLIST,
                       max_depth: int = 2, proxy: Optional[str] = None,
                       headers: dict = DEFAULT_HEADERS, delay: float = 0.5) -> List[str]:

    logging.info(f"[DIR-ENUM] Starting recursive directory enumeration on {base_url} with max depth {max_depth}")
    async with aiohttp.ClientSession() as session:
        found_dirs = await recursive_dir_enum(session, base_url, wordlist, max_depth, 0, set(), proxy, headers, delay)
    logging.info(f"[DIR-ENUM] Enumeration complete on {base_url}: {len(found_dirs)} directories found")
    return sorted(list(found_dirs))
